# Домашнее задание к занятию "Установка Kubernetes"

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развернутые ВМ с ОС Ubuntu 20.04-lts


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
2. [Документация kubespray](https://kubespray.io/)

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 3 нод: 1 мастер и 2 рабочие ноды.
![image](https://user-images.githubusercontent.com/99823951/235982707-9750fb19-d49a-4509-847e-e20dac486b70.png)  
Развернул 3 ВМ

2. В качестве CRI — containerd.  
3. Запуск etcd производить на мастере.  
4. Способ установки выбрать самостоятельно.  
Решил проводить установку через `kubeadm`  
Запускаю установку `kubelet kubeadm kubectl containerd` на всех нодах
```bash
ubuntu@nd1:~$ {
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl containerd
sudo apt-mark hold kubelet kubeadm kubectl
}
```
Произвожу инициализацию кластера. Указываю ip master-ноды, ip сети для pod-ов и ip-внешний (для генерация сертификата, чтобы подключаться из вне)  
```bash
ubuntu@nd1:~$ kubeadm init \
  --apiserver-advertise-address=10.129.0.8 \
  --pod-network-cidr 10.224.0.0/16 \
  --apiserver-cert-extra-sans=158.160.30.169 
```
Включаю `forwarding`, без него `kubeadm Init` не проходит
```bash
ubuntu@nd1:~$ sudo -i
root@nd1:~# modprobe br_netfilter
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-iptables=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-arptables=1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-ip6tables=1" >> /etc/sysctl.conf
sysctl -p /etc/sysctl.conf
```
Получаю токен для подключения воркеров к кластеру  
```bash
ubuntu@nd2:~$ sudo kubeadm join 10.129.0.8:6443 --token d77lac.98314r3c7ucdulpe         --discovery-token-ca-cert-hash sha256:a1a5e534e879ea80ec86c4b46d04506d4c3b090e5296bee9475571eba0ae76f1 
```
```bash
ubuntu@nd3:~$ sudo kubeadm join 10.129.0.8:6443 --token d77lac.98314r3c7ucdulpe         --discovery-token-ca-cert-hash sha256:a1a5e534e879ea80ec86c4b46d04506d4c3b090e5296bee9475571eba0ae76f1 

...........................

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

```
Копирую админ конфиг в профиль текущего пользователя, для возможности запуска `kubectl`  
```bash
ubuntu@nd1:~$ {
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
}
ubuntu@nd1:~$ kubectl get nodes
NAME   STATUS     ROLES           AGE     VERSION
nd1    NotReady   control-plane   7m29s   v1.27.1
nd2    NotReady   <none>          4m54s   v1.27.1
nd3    NotReady   <none>          4m10s   v1.27.1
```
Поиск проблемы через `describe nodes` выдал сообщение, что `not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized`. Поэтому идем устанавливать плагин сети  
```bash
ubuntu@nd1:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

5. Настроить подключение с локальной машины.
Переключаюсь на локальную машину, копирую конфиг себе в `~/.kube/config` и проверяем работу.
```bash
ubuntu@ubuntu-VirtualBox:~/Desktop$ kubectl get nodes
NAME     STATUS   ROLES           AGE   VERSION
node-1   Ready    control-plane   77m   v1.27.1
node-2   Ready    <none>          67m   v1.27.1
node-3   Ready    <none>          67m   v1.27.1

ubuntu@ubuntu-VirtualBox:~/.kube$ kubectl get pod -A
NAMESPACE      NAME                             READY   STATUS              RESTARTS        AGE
kube-flannel   kube-flannel-ds-brhqr            1/1     Running             0               67m
kube-flannel   kube-flannel-ds-js8rq            1/1     Running             0               67m
kube-flannel   kube-flannel-ds-nkt6c            1/1     Running             0               67m
kube-system    coredns-5d78c9869d-hrhbc         1/1     Running             0               76m
kube-system    coredns-5d78c9869d-z6qfv         1/1     Running             0               76m
kube-system    etcd-node-1                      1/1     Running             0               74m
kube-system    kube-apiserver-node-1            1/1     Running             0               77m
kube-system    kube-controller-manager-node-1   1/1     Running             0               75m
kube-system    kube-proxy-r5tf4                 1/1     Running             0               76m
kube-system    kube-proxy-v9f9p                 1/1     Running             0               67m
kube-system    kube-proxy-549d4                 1/1     Running             0               75m
kube-system    kube-scheduler-node-1            1/1     Running             0               75m

```



```bash
buntu@ubuntu-VirtualBox:~/kubespray$ ssh 158.160.0.124
Warning: Permanently added '158.160.0.124' (ED25519) to the list of known hosts.
Welcome to Ubuntu 22.04.2 LTS (GNU/Linux 5.15.0-71-generic x86_64)


ubuntu@node-1:~$ git clone https://github.com/kubernetes-sigs/kubespray
Cloning into 'kubespray'...
remote: Enumerating objects: 67607, done.
remote: Counting objects: 100% (331/331), done.
remote: Compressing objects: 100% (194/194), done.
remote: Total 67607 (delta 140), reused 273 (delta 108), pack-reused 67276
Receiving objects: 100% (67607/67607), 21.56 MiB | 21.16 MiB/s, done.
Resolving deltas: 100% (38025/38025), done.


ubuntu@node-1:~/kubespray$ sudo apt install pip

ubuntu@node-1:~/kubespray$ sudo pip3 install -r requirements.txt


ubuntu@node-1:~/kubespray$ cp -rfp inventory/sample/ inventory/mycluster


ubuntu@node-1:~/kubespray$ declare -a IPS=(10.129.0.26 10.129.0.18)
ubuntu@node-1:~/kubespray$ CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}


ubuntu@nd-1:~/kubespray$ cat inventory/mycluster/hosts.yaml
all:
  hosts:
    node1:
      ansible_host: 10.129.0.26
      ip: 10.129.0.26
      access_ip: 10.129.0.26
    node2:
      ansible_host: 10.129.0.18
      ip: 10.129.0.18
      access_ip: 10.129.0.18
  children:
    kube_control_plane:
      hosts:
        node1:
    kube_node:
      hosts:
        node1:
        node2:
    etcd:
      hosts:
        node1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}



ubuntu@nd-1:~/kubespray$ nano inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml 

# supplementary_addresses_in_ssl_keys: [158.160.0.124]

.ssh id_rsa

ubuntu@nd-1:~/kubespray$ ansible-playbook -i inventory/mycluster/hosts.yaml cluster.yml -b -v


ubuntu@nd-1:~/kubespray$ 
{
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
}


ubuntu@nd-1:~/kubespray$ kubectl get nodes
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   12m   v1.26.3
node2   Ready    <none>          11m   v1.26.3
```
